"use client"

import { useState, useEffect } from "react"

// AIæ¨¡å‹ç®¡ç†ä¸­å¿ƒ - ç»Ÿä¸€ç®¡ç†æœ¬åœ°å’Œäº‘ç«¯AIæ¨¡å‹
export class ModelManagementCenter {
  private static instance: ModelManagementCenter
  private models = new Map<string, AIModel>()
  private modelTasks = new Map<string, ModelTask>()
  private config: ModelManagementConfig

  private constructor() {
    this.config = {
      ollamaUrl: process.env.NEXT_PUBLIC_OLLAMA_URL || "http://localhost:11434",
      modelCacheDir: "/tmp/yanyu-models",
      defaultModels: [
        { id: "codellama:7b", name: "CodeLlama 7B", type: "code", provider: "ollama" },
        { id: "llama3:8b", name: "Llama 3 8B", type: "chat", provider: "ollama" },
        { id: "phi3:mini", name: "Phi-3 Mini", type: "chat", provider: "ollama" },
      ],
      autoDownload: false,
      maxConcurrentDownloads: 1,
    }

    this.initializeModels()
  }

  public static getInstance(): ModelManagementCenter {
    if (!ModelManagementCenter.instance) {
      ModelManagementCenter.instance = new ModelManagementCenter()
    }
    return ModelManagementCenter.instance
  }

  // åˆå§‹åŒ–æ¨¡å‹åˆ—è¡¨
  private async initializeModels(): Promise<void> {
    try {
      // è·å–æœ¬åœ°å·²å®‰è£…çš„Ollamaæ¨¡å‹
      await this.fetchOllamaModels()

      // æ·»åŠ é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
      for (const defaultModel of this.config.defaultModels) {
        if (!this.models.has(defaultModel.id)) {
          this.models.set(defaultModel.id, {
            ...defaultModel,
            status: "not_downloaded",
            size: 0,
            lastUsed: null,
            parameters: defaultModel.id.includes("7b") ? "7B" : defaultModel.id.includes("8b") ? "8B" : "Unknown",
            quantization: defaultModel.id.includes("q4_0") ? "Q4_0" : "None",
            createdAt: new Date(),
          })
        }
      }

      console.log(`âœ… å·²åˆå§‹åŒ–${this.models.size}ä¸ªAIæ¨¡å‹`)
    } catch (error) {
      console.error("åˆå§‹åŒ–æ¨¡å‹å¤±è´¥:", error)
    }
  }

  // è·å–Ollamaæ¨¡å‹åˆ—è¡¨
  private async fetchOllamaModels(): Promise<void> {
    try {
      const response = await fetch(`${this.config.ollamaUrl}/api/tags`)

      if (!response.ok) {
        throw new Error(`è·å–Ollamaæ¨¡å‹å¤±è´¥: ${response.status}`)
      }

      const data = await response.json()

      if (data.models) {
        for (const model of data.models) {
          const modelId = model.name
          const existingModel = this.models.get(modelId)

          this.models.set(modelId, {
            id: modelId,
            name: this.formatModelName(modelId),
            type: this.inferModelType(modelId),
            provider: "ollama",
            status: "ready",
            size: model.size || 0,
            lastUsed: existingModel?.lastUsed || null,
            parameters: this.inferModelParameters(modelId),
            quantization: this.inferModelQuantization(modelId),
            createdAt: existingModel?.createdAt || new Date(),
            updatedAt: new Date(),
          })
        }
      }
    } catch (error) {
      console.error("è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥:", error)
      // å¦‚æœè·å–å¤±è´¥ï¼Œå¯èƒ½æ˜¯OllamaæœåŠ¡æœªå¯åŠ¨ï¼Œæ ‡è®°æ‰€æœ‰Ollamaæ¨¡å‹ä¸ºæœªçŸ¥çŠ¶æ€
      // ä½¿ç”¨Array.fromå°†Mapè½¬æ¢ä¸ºæ•°ç»„ï¼Œé¿å…ä½¿ç”¨MapIterator
      for (const [id, model] of Array.from(this.models.entries())) {
        if (model.provider === "ollama") {
          this.models.set(id, { ...model, status: "unknown" })
        }
      }
    }
  }

  // æ ¼å¼åŒ–æ¨¡å‹åç§°
  private formatModelName(modelId: string): string {
    // å°†æ¨¡å‹IDè½¬æ¢ä¸ºæ›´å‹å¥½çš„æ˜¾ç¤ºåç§°
    const parts = modelId.split(":")
    const baseName = parts[0]
    const version = parts[1] || ""

    const nameMap: Record<string, string> = {
      codellama: "CodeLlama",
      llama3: "Llama 3",
      llama2: "Llama 2",
      phi3: "Phi-3",
      mistral: "Mistral",
      qwen: "Qwen",
      gemma: "Gemma",
    }

    const formattedName = nameMap[baseName] || baseName.charAt(0).toUpperCase() + baseName.slice(1)

    if (version) {
      return `${formattedName} ${version}`
    }

    return formattedName
  }

  // æ¨æ–­æ¨¡å‹ç±»å‹
  private inferModelType(modelId: string): ModelType {
    const id = modelId.toLowerCase()

    if (id.includes("code") || id.includes("starcoder") || id.includes("deepseek-coder")) {
      return "code"
    } else if (id.includes("vision") || id.includes("clip") || id.includes("image")) {
      return "multimodal"
    } else {
      return "chat"
    }
  }

  // æ¨æ–­æ¨¡å‹å‚æ•°é‡
  private inferModelParameters(modelId: string): string {
    const id = modelId.toLowerCase()

    if (id.includes("70b")) return "70B"
    if (id.includes("34b")) return "34B"
    if (id.includes("13b")) return "13B"
    if (id.includes("8b")) return "8B"
    if (id.includes("7b")) return "7B"
    if (id.includes("3b")) return "3B"
    if (id.includes("1b")) return "1B"

    return "æœªçŸ¥"
  }

  // æ¨æ–­æ¨¡å‹é‡åŒ–æ–¹å¼
  private inferModelQuantization(modelId: string): string {
    const id = modelId.toLowerCase()

    if (id.includes("q4_0")) return "Q4_0"
    if (id.includes("q4_1")) return "Q4_1"
    if (id.includes("q5_0")) return "Q5_0"
    if (id.includes("q5_1")) return "Q5_1"
    if (id.includes("q8_0")) return "Q8_0"

    return "æ— é‡åŒ–"
  }

  // è·å–æ‰€æœ‰æ¨¡å‹
  public getAllModels(): AIModel[] {
    return Array.from(this.models.values())
  }

  // è·å–ç‰¹å®šç±»å‹çš„æ¨¡å‹
  public getModelsByType(type: ModelType): AIModel[] {
    return Array.from(this.models.values()).filter((model) => model.type === type)
  }

  // è·å–å¯ç”¨çš„æ¨¡å‹
  public getAvailableModels(): AIModel[] {
    return Array.from(this.models.values()).filter((model) => model.status === "ready")
  }

  // è·å–æ¨¡å‹è¯¦æƒ…
  public getModel(modelId: string): AIModel | undefined {
    return this.models.get(modelId)
  }

  // ä¸‹è½½æ¨¡å‹
  public async downloadModel(modelId: string, onProgress?: (progress: number, status: string) => void): Promise<ModelTask> {
    // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸‹è½½ä»»åŠ¡
    let task = this.modelTasks.get(modelId)
    if (task && ["pending", "downloading"].includes(task.status)) {
      return task
    }

    // åˆ›å»ºæ–°çš„ä¸‹è½½ä»»åŠ¡
    task = {
      id: `task_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      modelId,
      type: "download",
      status: "pending",
      progress: 0,
      createdAt: new Date(),
      updatedAt: new Date(),
    }

    this.modelTasks.set(modelId, task)

    // æ›´æ–°æ¨¡å‹çŠ¶æ€
    const model = this.models.get(modelId)
    if (model) {
      this.models.set(modelId, { ...model, status: "downloading" })
    }
    
    // è°ƒç”¨è¿›åº¦å›è°ƒ
    onProgress?.(0, 'å‡†å¤‡ä¸‹è½½...')

    // å¼€å§‹ä¸‹è½½
    this.startModelDownload(modelId, task, onProgress)

    return task
  }

  // å¼€å§‹æ¨¡å‹ä¸‹è½½
  private async startModelDownload(modelId: string, task: ModelTask, onProgress?: (progress: number, status: string) => void): Promise<void> {
    try {
      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "downloading"
      task.startedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      console.log(`ğŸ”„ å¼€å§‹ä¸‹è½½æ¨¡å‹: ${modelId}`)
      
      // è°ƒç”¨è¿›åº¦å›è°ƒ
      onProgress?.(0, `å¼€å§‹ä¸‹è½½æ¨¡å‹: ${modelId}`)

      // è°ƒç”¨Ollama APIä¸‹è½½æ¨¡å‹
      const response = await fetch(`${this.config.ollamaUrl}/api/pull`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ name: modelId }),
      })

      if (!response.ok) {
        throw new Error(`ä¸‹è½½æ¨¡å‹å¤±è´¥: ${response.status}`)
      }

      // Ollama APIè¿”å›çš„æ˜¯æµå¼å“åº”ï¼Œéœ€è¦é€è¡Œè¯»å–
      const reader = response.body?.getReader()
      if (!reader) {
        throw new Error("æ— æ³•è¯»å–å“åº”æµ")
      }

      let receivedLength = 0
      let totalLength = 0

      while (true) {
        const { done, value } = await reader.read()

        if (done) {
          break
        }

        // è§£æè¿›åº¦ä¿¡æ¯
        const text = new TextDecoder().decode(value)
        const lines = text.split("\n").filter((line) => line.trim())

        for (const line of lines) {
          try {
            const data = JSON.parse(line)

            if (data.total && data.completed) {
              totalLength = data.total
              receivedLength = data.completed

              const progress = Math.round((receivedLength / totalLength) * 100)

              // æ›´æ–°ä»»åŠ¡è¿›åº¦
              task.progress = progress
              task.updatedAt = new Date()
              this.modelTasks.set(modelId, { ...task })

              // æ›´æ–°æ¨¡å‹çŠ¶æ€
              const model = this.models.get(modelId)
              if (model) {
                this.models.set(modelId, {
                  ...model,
                  status: "downloading",
                  downloadProgress: progress,
                })
              }
              
              // è°ƒç”¨è¿›åº¦å›è°ƒ
              onProgress?.(progress, `ä¸‹è½½ä¸­... ${progress}%`)
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }

      // ä¸‹è½½å®Œæˆï¼Œæ›´æ–°çŠ¶æ€
      task.status = "completed"
      task.progress = 100
      task.completedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      await this.fetchOllamaModels() // é‡æ–°è·å–æ¨¡å‹åˆ—è¡¨ä»¥æ›´æ–°çŠ¶æ€

      console.log(`âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: ${modelId}`)
      
      // å®Œæˆå›è°ƒ
      onProgress?.(100, 'ä¸‹è½½å®Œæˆ')
    } catch (error) {
      console.error(`âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: ${modelId}`, error)

      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "failed"
      task.error = error instanceof Error ? error.message : "ä¸‹è½½å¤±è´¥"
      task.updatedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      const model = this.models.get(modelId)
      if (model) {
        this.models.set(modelId, { ...model, status: "download_failed" })
      }
      
      // å¤±è´¥å›è°ƒ
      onProgress?.(0, `ä¸‹è½½å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`)
    }
  }

  // åˆ é™¤æ¨¡å‹
  public async deleteModel(modelId: string): Promise<boolean> {
    try {
      // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
      const model = this.models.get(modelId)
      if (!model) {
        throw new Error(`æ¨¡å‹ä¸å­˜åœ¨: ${modelId}`)
      }

      // åªèƒ½åˆ é™¤Ollamaæ¨¡å‹
      if (model.provider !== "ollama") {
        throw new Error(`ä¸æ”¯æŒåˆ é™¤éOllamaæ¨¡å‹: ${modelId}`)
      }

      console.log(`ğŸ—‘ï¸ å¼€å§‹åˆ é™¤æ¨¡å‹: ${modelId}`)

      // è°ƒç”¨Ollama APIåˆ é™¤æ¨¡å‹
      const response = await fetch(`${this.config.ollamaUrl}/api/delete`, {
        method: "DELETE",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ name: modelId }),
      })

      if (!response.ok) {
        throw new Error(`åˆ é™¤æ¨¡å‹å¤±è´¥: ${response.status}`)
      }

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      this.models.set(modelId, { ...model, status: "not_downloaded" })

      console.log(`âœ… æ¨¡å‹åˆ é™¤æˆåŠŸ: ${modelId}`)
      return true
    } catch (error) {
      console.error(`âŒ æ¨¡å‹åˆ é™¤å¤±è´¥: ${modelId}`, error)
      return false
    }
  }

  // è·å–æ¨¡å‹ä»»åŠ¡
  public getModelTasks(): ModelTask[] {
    return Array.from(this.modelTasks.values())
  }

  // è·å–æ¨¡å‹ä»»åŠ¡è¯¦æƒ…
  public getModelTask(taskId: string): ModelTask | undefined {
    // ä½¿ç”¨Array.fromå°†Mapè½¬æ¢ä¸ºæ•°ç»„ï¼Œé¿å…ä½¿ç”¨MapIterator
    for (const task of Array.from(this.modelTasks.values())) {
      if (task.id === taskId) {
        return task
      }
    }
    return undefined
  }

  // è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
  public getModelStats(): ModelStats {
    const models = this.getAllModels()

    return {
      total: models.length,
      ready: models.filter((m) => m.status === "ready").length,
      downloading: models.filter((m) => m.status === "downloading").length,
      notDownloaded: models.filter((m) => m.status === "not_downloaded").length,
      byType: {
        chat: models.filter((m) => m.type === "chat").length,
        code: models.filter((m) => m.type === "code").length,
        multimodal: models.filter((m) => m.type === "multimodal").length,
      },
      byProvider: {
        ollama: models.filter((m) => m.provider === "ollama").length,
        openai: models.filter((m) => m.provider === "openai").length,
        anthropic: models.filter((m) => m.provider === "anthropic").length,
        google: models.filter((m) => m.provider === "google").length,
      },
      totalSize: models.reduce((sum, m) => sum + (m.size || 0), 0),
    }
  }

  // åˆ·æ–°æ¨¡å‹åˆ—è¡¨
  public async refreshModels(): Promise<void> {
    await this.fetchOllamaModels()
  }

  // ä½¿ç”¨æ¨¡å‹ï¼ˆè®°å½•ä½¿ç”¨æ—¶é—´ï¼‰
  public useModel(modelId: string): void {
    const model = this.models.get(modelId)
    if (model) {
      this.models.set(modelId, {
        ...model,
        lastUsed: new Date(),
        usageCount: (model.usageCount || 0) + 1,
      })
    }
  }

  // è·å–æ¨èæ¨¡å‹
  public getRecommendedModels(type: ModelType = "chat", count = 3): AIModel[] {
    const availableModels = this.getAvailableModels().filter((m) => m.type === type)

    // æŒ‰ä½¿ç”¨æ¬¡æ•°å’Œæœ€è¿‘ä½¿ç”¨æ—¶é—´æ’åº
    return availableModels
      .sort((a, b) => {
        // é¦–å…ˆæŒ‰ä½¿ç”¨æ¬¡æ•°æ’åº
        const countDiff = (b.usageCount || 0) - (a.usageCount || 0)
        if (countDiff !== 0) return countDiff

        // å…¶æ¬¡æŒ‰æœ€è¿‘ä½¿ç”¨æ—¶é—´æ’åº
        if (a.lastUsed && b.lastUsed) {
          return b.lastUsed.getTime() - a.lastUsed.getTime()
        }

        return a.lastUsed ? -1 : b.lastUsed ? 1 : 0
      })
      .slice(0, count)
  }
}

// ç±»å‹å®šä¹‰
export type ModelType = "chat" | "code" | "multimodal"
export type ModelProvider = "ollama" | "openai" | "anthropic" | "google"
export type ModelStatus = "ready" | "downloading" | "not_downloaded" | "download_failed" | "unknown"
export type TaskType = "download" | "update" | "delete"
export type TaskStatus = "pending" | "downloading" | "completed" | "failed"

export interface AIModel {
  id: string
  name: string
  type: ModelType
  provider: ModelProvider
  status: ModelStatus
  size: number
  lastUsed: Date | null
  parameters: string
  quantization: string
  createdAt: Date
  updatedAt?: Date
  downloadProgress?: number
  usageCount?: number
}

export interface ModelTask {
  id: string
  modelId: string
  type: TaskType
  status: TaskStatus
  progress: number
  createdAt: Date
  updatedAt: Date
  startedAt?: Date
  completedAt?: Date
  error?: string
}

export interface ModelManagementConfig {
  ollamaUrl: string
  modelCacheDir: string
  defaultModels: Array<{
    id: string
    name: string
    type: ModelType
    provider: ModelProvider
  }>
  autoDownload: boolean
  maxConcurrentDownloads: number
}

export interface ModelStats {
  total: number
  ready: number
  downloading: number
  notDownloaded: number
  byType: Record<ModelType, number>
  byProvider: Record<ModelProvider, number>
  totalSize: number
}

// å¯¼å‡ºæ¨¡å‹ç®¡ç†ä¸­å¿ƒå®ä¾‹
export const modelManagementCenter = ModelManagementCenter.getInstance()

// React Hook - ä½¿ç”¨æ¨¡å‹ç®¡ç†ä¸­å¿ƒ
export function useModelManagement() {
  const [models, setModels] = useState<AIModel[]>([])
  const [tasks, setTasks] = useState<ModelTask[]>([])
  const [stats, setStats] = useState<ModelStats | null>(null)
  const [loading, setLoading] = useState(true)

  useEffect(() => {
    // åˆå§‹åŠ è½½
    loadModels()

    // å®šæœŸåˆ·æ–°
    const interval = setInterval(() => {
      loadModels()
    }, 5000)

    return () => clearInterval(interval)
  }, [])

  const loadModels = async () => {
    try {
      await modelManagementCenter.refreshModels()
      setModels(modelManagementCenter.getAllModels())
      setTasks(modelManagementCenter.getModelTasks())
      setStats(modelManagementCenter.getModelStats())
      setLoading(false)
    } catch (error) {
      console.error("åŠ è½½æ¨¡å‹å¤±è´¥:", error)
      setLoading(false)
    }
  }

  const downloadModel = async (modelId: string) => {
    await modelManagementCenter.downloadModel(modelId)
    loadModels()
  }

  const deleteModel = async (modelId: string) => {
    await modelManagementCenter.deleteModel(modelId)
    loadModels()
  }

  return {
    models,
    tasks,
    stats,
    loading,
    refreshModels: loadModels,
    downloadModel,
    deleteModel,
    getModelsByType: (type: ModelType) => modelManagementCenter.getModelsByType(type),
    getAvailableModels: () => modelManagementCenter.getAvailableModels(),
    getRecommendedModels: (type?: ModelType, count?: number) => modelManagementCenter.getRecommendedModels(type, count),
  }
}
